# -*- coding: utf-8 -*-
"""ML2_Assignment_3_Solution_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fo5eUnR8hUcyWKbuIFtlc7jognIIf1Dk
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install torchtext==0.6.0
!wget https://www.cl.uni-heidelberg.de/statnlpgroup/decoco/ms_coco_parallel.tar.bz2
!gunzip *.bz2
!bunzip2 ms_coco_parallel.tar.bz2
!tar -xvf *.tar
# %cd ms_coco_parallel/


# !cat dev.en devtest.en test.en > en_data.txt
# !cat dev.de devtest.de test.de > de_data.txt

!ls

list_train_de=[]
filepath="dev.de"
with open(filepath, encoding="utf8") as fp:  
   for line in fp:
       list_train_de.extend(line.strip().split(', '))

list_train_en=[]
filepath="dev.en"
with open(filepath, encoding="utf8") as fp:  
   for line in fp:
       list_train_en.extend(line.strip().split(', '))

list_test_de=[]
filepath="test.de"
with open(filepath, encoding="utf8") as fp:  
   for line in fp:
       list_test_de.extend(line.strip().split(', '))

list_test_en=[]
filepath="test.en"
with open(filepath, encoding="utf8") as fp:  
   for line in fp:
       list_test_en.extend(line.strip().split(', '))

# Lets play with csv and text
# try to convert text/arrays to csv and make it our code firendly!!!
# Source = English
# Target = German / Dutch
import pandas as pd
train = pd.DataFrame(list(zip(list_train_en, list_train_de)), 
               columns =['trg', 'src'])

test =  pd.DataFrame(list(zip(list_test_en, list_test_de)), 
               columns =['trg', 'src'])

train.head

test.head

train.to_csv("train.csv", index=False)
test.to_csv("test.csv", index=False)

!ls

import torch
import torch.nn as nn
import torch.optim as optim
import spacy
from utils import translate_sentence, bleu, save_checkpoint, load_checkpoint
import pandas as pd
from torchtext.data import Field, BucketIterator, TabularDataset
from sklearn.model_selection import train_test_split
from torch.utils.tensorboard import SummaryWriter
from torchtext.data import Field, BucketIterator

!python -m spacy download en
!python -m spacy download de


spacy_eng = spacy.load("en")
spacy_ger = spacy.load("de")


def tokenize_eng(text):
    return [tok.text for tok in spacy_eng.tokenizer(text)]


def tokenize_ger(text):
    return [tok.text for tok in spacy_ger.tokenizer(text)]


english = Field(sequential=True, use_vocab=True, tokenize=tokenize_eng, lower=True)
german = Field(sequential=True, use_vocab=True, tokenize=tokenize_ger, lower=True)

fields = {"trg": ("trg", english), "src": ("src", german)}

train_data, test_data = TabularDataset.splits(
    path="", train="train.csv", test="test.csv", format="csv", fields=fields
)

english.build_vocab(train_data, max_size=10000, min_freq=2)
german.build_vocab(train_data, max_size=10000, min_freq=2)

train_iterator, test_iterator = BucketIterator.splits(
    (train_data, test_data), batch_size=32, device="cuda"
)

for batch in train_iterator:
    print(batch)

# string to integer (stoi)
print(f'Index of the word (the) is: {english.vocab.stoi["the"]}')

# print integer to string (itos)
print(f"Word of the index (16) is: {english.vocab.itos[16]}")
print(f"Word of the index (17) is: {english.vocab.itos[17]}")
print(f"Word of the index (18) is: {english.vocab.itos[18]}")
print(f"Word of the index (19) is: {english.vocab.itos[19]}")
print(f"Word of the index (20) is: {english.vocab.itos[20]}")
print(f"Word of the index (21) is: {english.vocab.itos[21]}")
print(f"Word of the index (0) is: {english.vocab.itos[0]}")

!pip install torchtext==0.6.0
import torch
from torchtext.data.metrics import bleu_score
import spacy



def translate_sentence(model, sentence, german, english, device, max_length=50):
    # Load german tokenizer
    spacy_ger = spacy.load("de")

    # Create tokens using spacy and everything in lower case (which is what our vocab is)
    if type(sentence) == str:
        tokens = [token.text.lower() for token in spacy_ger(sentence)]
    else:
        tokens = [token.lower() for token in sentence]

    # Add <SOS> and <EOS> in beginning and end respectively
    tokens.insert(0, german.init_token)
    tokens.append(german.eos_token)

    # Go through each german token and convert to an index
    text_to_indices = [german.vocab.stoi[token] for token in tokens]

    # Convert to Tensor
    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)

    outputs = [english.vocab.stoi["<sos>"]]
    for i in range(max_length):
        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)

        with torch.no_grad():
            output = model(sentence_tensor, trg_tensor)

        best_guess = output.argmax(2)[-1, :].item()
        outputs.append(best_guess)

        if best_guess == english.vocab.stoi["<eos>"]:
            break

    translated_sentence = [english.vocab.itos[idx] for idx in outputs]
    # remove start token
    return translated_sentence[1:]

def bleu(data, model, german, english, device):
    targets= []
    outputs= []

    for eg in data:
        src = vars(eg)['src']
        trg = vars(eg)['trg']

        prediction = translate_sentence(model, src, german, english, device)
        prediction = prediction[:-1] # eos removed

        targets.append([trg])
        outputs.append(prediction)

    return bleu_score(outputs, targets)

# Senetence : Giraffen an einem sonnigen Tag im Busch.
# Converted Expected : Giraffes in the forest on a sunny day



class Transformer(nn.Module):
    def __init__(self,
                embedding_size,
                src_vocab_size,
                trg_vocab_size,
                src_pad_idx,
                num_heads,
                num_encoder_layers,
                num_decoder_layers,
                forward_expansion,
                dropout,
                max_len,
                device
    ):
        super(Transformer,self).__init__()
        self.src_word_embedding = nn.Embedding(src_vocab_size,embedding_size)
        self.src_positional_embedding = nn.Embedding(max_len,embedding_size)
        self.trg_word_embedding =  nn.Embedding(trg_vocab_size,embedding_size)
        self.trg_positional_embedding = nn.Embedding(max_len, embedding_size)

        self.device = device
        self.transformer = nn.Transformer(
                    embedding_size,
                    num_heads,
                    num_encoder_layers,
                    num_decoder_layers,
                    forward_expansion,
                    dropout
        )
        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)
        self.dropout = nn.Dropout(dropout)
        self.src_pad_idx = src_pad_idx

    def make_src_masks(self,src):
        # src : (src_len, bs)
        src_mask = src.transpose(0,1) == self.src_pad_idx
        # (bs, src_len)
        return src_mask

    def forward(self, src, trg):
        src_seq_len , N = src.shape
        trg_seq_len , N = trg.shape

        src_positions =(
            torch.arange(0, src_seq_len).unsqueeze(1).expand(src_seq_len, N).to(self.device)
        )
        trg_positions = (
            torch.arange(0, trg_seq_len).unsqueeze(1).expand(trg_seq_len, N).to(self.device)
        )

        embed_src = self.dropout(
            (self.src_word_embedding(src) + self.src_positional_embedding(src_positions))
        )
        embed_trg = self.dropout(
            (self.trg_word_embedding(trg) + self.trg_positional_embedding(trg_positions))
        )
        src_padding_mask = self.make_src_masks(src)
        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_len).to(self.device)

        out = self.transformer(embed_src, embed_trg, src_key_padding_mask=src_padding_mask,tgt_mask=trg_mask)

        out = self.fc_out(out)
        return out

#
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
load_model =True
save_model = False

#training hyperparameters
num_epochs = 100
learning_rate = 3e-4
batch_size= 32

#model hyperparameters
src_vocab_size = len(german.vocab)
trg_vocab_size = len(english.vocab)
embedding_size = 512
num_heads = 8
num_encoder_layers = 3
num_decoder_layers = 3
dropout = 0.1
max_len = 100
forward_expansion = 4
src_pad_idx = english.vocab.stoi['<pad>']

writer = SummaryWriter("runs/loss_plot")
step = 0

train_iterator, valid_iterator, test_iterator = BucketIterator.splits(
        (train_data, validation_data, test_data),
        batch_size = batch_size,
        sort_within_batch=True,
        sort_key = lambda x: len(x.src),
        device = device)

model = Transformer(embedding_size,
            src_vocab_size,
            trg_vocab_size,
            src_pad_idx,
            num_heads,
            num_encoder_layers,
            num_decoder_layers,
            forward_expansion,
            dropout,
            max_len,
            device).to(device)

optimizer = optim.Adam(model.parameters(),lr = learning_rate)

pad_idx = english.vocab.stoi["<pad>"]
criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)

'''if load_model:
    load_checkpoint(torch.load('my_checkpoint.pth.tar',map_location=torch.device('cpu')), model, optimizer)
    print("Loaded")'''

train = True
if train:
    for epoch in range(num_epochs):
        print(f'[Epoch{epoch+1}/{num_epochs}]')

        if save_model:
            checkpoint= {
            "state_dict" : model.state_dict(),
            "optimizer" : optimizer.state_dict(),
            }
        #save_checkpoint(checkpoint)

        for batch_idx,batch in enumerate(train_iterator):
            inp_data = batch.src.to(device)
            target = batch.trg.to(device)

            #forward prop
            output = model(inp_data, target[:-1])

            output = output.reshape(-1,output.shape[2])
            target = target[1:].reshape(-1)

            optimizer.zero_grad()

            loss = criterion(output, target)
            loss.backward()

            torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm =1)

            optimizer.step()

            writer.add_scalar("Training loss", loss, global_step=step)
            step+=1
sen1 = "Giraffen an einem sonnigen Tag im Busch."

a = translate_sentence(model, sen1, german,english, device, max_length=30)

print("English")
print(a)


import sys
sys.argv=['']
del sys

score = bleu(test_data[1:150],model,german,english,device)
print("score:")
print(score)

sen1 = "Alle die wandern sind nicht verloren"
# English Original: All those who wander are not lost
a = translate_sentence(model, sen1, german,english, device, max_length=30)

print("English")
print(a)